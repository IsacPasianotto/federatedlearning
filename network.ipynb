{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:09:38.526179Z",
     "start_time": "2024-07-01T10:09:38.522550Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee82207-8b09-4c97-be8b-e4f725e8885e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:03:46.595694Z",
     "start_time": "2024-07-01T10:03:46.591884Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    overload dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        # enforce to use of float32 or some models will complain\n",
    "        self.x = self.x.float()\n",
    "        self.y = self.y.float()\n",
    "\n",
    "    def __len__ (self): \n",
    "        return(len(self.y))\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cd564454eb7ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:03:46.720407Z",
     "start_time": "2024-07-01T10:03:46.597549Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = torch.load('data/brain_cancer_dataset_small.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6012ec34a1726fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:03:46.726076Z",
     "start_time": "2024-07-01T10:03:46.722696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4964f602b0b5cf63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:03:46.755446Z",
     "start_time": "2024-07-01T10:03:46.746476Z"
    }
   },
   "outputs": [],
   "source": [
    "class BrainClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrainClassifier, self).__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Calculate the input size of the fully connected layer according to the output of the conv layers\n",
    "        fc_input_size = 64 * 64 * 64\n",
    "        self.fc = nn.Linear(fc_input_size, 3) # 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        # by default CrossEntropyLoss compute the softmax itself.\n",
    "        # if leaved uncommented somewhy (????) the model does not learn \n",
    "        # remeber to put by hand when you do predictions\n",
    "        # x = F.softmax(x, dim=1)  \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e3a865-323f-489c-b4a9-bab3c8ff4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class minst_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(minst_classifier, self).__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the flattened size\n",
    "        self.flatten_size = 64 * 3 * 3  # Adjusted from 4*4 to 3*3\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(self.flatten_size, 10)  # Adjust accordingly\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c5deb61e83d28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:03:46.770594Z",
     "start_time": "2024-07-01T10:03:46.763063Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train(model, train_data, val_data, epochs=10, lr=0.001):\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Store the losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Move model to the appropriate device\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training data\n",
    "        for i, (img, label) in enumerate(train_data):\n",
    "            # Move data to the appropriate device\n",
    "            img, label = img.to(device), label.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = model(img)\n",
    "            # Compute the loss\n",
    "            loss = criterion(y_pred, label.long() - 1)\n",
    "            # loss = criterion(y_pred, label.long())\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            # Store the loss\n",
    "            train_losses.append(loss.item())\n",
    "            # Print the loss\n",
    "            # if i % 10 == 0:\n",
    "            #     print(f'Epoch {epoch}, Iteration {i}, Loss {loss.item()}')\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        # Compute the validation loss\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for img, label in val_data:\n",
    "                # Move data to the appropriate device\n",
    "                img, label = img.to(device), label.to(device)\n",
    "                y_pred = model(img)\n",
    "                loss = criterion(y_pred, label.long() - 1)\n",
    "                # loss = criterion(y_pred, label.long())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_data))\n",
    "\n",
    "        print(f'Epoch {epoch}, Validation Loss {val_loss / len(val_data)}')\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c66ac8e8f19dcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:03:46.851669Z",
     "start_time": "2024-07-01T10:03:46.779892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),  # Converts to float32 and scales to [0, 1]\n",
    "#     transforms.Normalize((0.5,), (0.5,))  # Normalize with mean 0.5 and std 0.5 (example values, adjust if needed)\n",
    "# ])\n",
    "\n",
    "# Load the dataset\n",
    "# train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "# val_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# # Create data loaders\n",
    "train_loader = DataLoader(data, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(data, batch_size=128, shuffle=False)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "440cc3f9e9008b8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:08:49.203801Z",
     "start_time": "2024-07-01T10:03:46.853279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation Loss 517.2254943847656\n",
      "Epoch 1, Validation Loss 17.13946835199992\n",
      "Epoch 2, Validation Loss 6.311733945611347\n",
      "Epoch 3, Validation Loss 0.9515390495459238\n",
      "Epoch 4, Validation Loss 0.44365549087524414\n",
      "Epoch 5, Validation Loss 0.7678773974378904\n",
      "Epoch 6, Validation Loss 0.04298717404405276\n",
      "Epoch 7, Validation Loss 0.026183693689138938\n",
      "Epoch 8, Validation Loss 0.020321732852607965\n",
      "Epoch 9, Validation Loss 0.016199760449429352\n"
     ]
    }
   ],
   "source": [
    "# Example instantiation and training\n",
    "model = BrainClassifier()\n",
    "# model = minst_classifier()\n",
    "train_losses, val_losses = train(model, train_loader, val_loader, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accdc5d14f252460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:11:41.023967Z",
     "start_time": "2024-07-01T10:11:40.801571Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot the losses\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# plot the losses\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xlim(0, 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1f2db460dfa84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:12:14.971910Z",
     "start_time": "2024-07-01T10:12:14.969083Z"
    }
   },
   "outputs": [],
   "source": [
    "# giusto pre vedere che cos'è il dataset usato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540911872298477b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:14:38.185052Z",
     "start_time": "2024-07-01T10:14:38.173761Z"
    }
   },
   "outputs": [],
   "source": [
    "type(train_dataset)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68509ef5cd2812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:16:05.903548Z",
     "start_time": "2024-07-01T10:16:05.898864Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the dtype of the data\n",
    "train_dataset[0][0].dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
